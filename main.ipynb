{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Load the Training data and labels!!!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 23 12:58:28 2019\n",
    "\n",
    "@author: fame\n",
    "\"\"\" \n",
    "import os  \n",
    "import torch\n",
    "import numpy as np\n",
    "import os.path \n",
    " \n",
    " \n",
    "def _isArrayLike(obj):\n",
    "    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\n",
    "\n",
    " \n",
    "def load_data(split_load, actions_dict, GT_folder, DATA_folder, features_folder, datatype = 'training'):\n",
    "    file_ptr = open(split_load, 'r')\n",
    "    content_all = file_ptr.read().split('\\n')[1:-1]\n",
    "    content_all = [x.strip('./data/groundTruth/') + 't' for x in content_all]\n",
    "    all_tasks = ['tea', 'cereals', 'coffee', 'friedegg', 'juice', 'milk', 'sandwich', 'scrambledegg', 'pancake', 'salat']\n",
    "    \n",
    "    if datatype == 'training':\n",
    "        data_breakfast = []\n",
    "        labels_breakfast = []\n",
    "        for content in content_all:\n",
    "        \n",
    "            file_ptr = open( GT_folder + content, 'r')\n",
    "            curr_gt = file_ptr.read().split('\\n')[:-1]\n",
    "            loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n",
    "            curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n",
    "            feature_name = features_folder + os.path.splitext(content)[0] + '.npy'\n",
    "            np.save(feature_name, curr_data)\n",
    "            label_curr_video = []\n",
    "            for iik in range(len(curr_gt)):\n",
    "                label_curr_video.append( actions_dict[curr_gt[iik]] )\n",
    "\n",
    "            data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n",
    "            labels_breakfast.append(label_curr_video )\n",
    "    \n",
    "        labels_uniq, labels_uniq_loc = get_label_bounds(labels_breakfast)\n",
    "        print(\"Finish Load the Training data and labels!!!\")     \n",
    "        return  data_breakfast, labels_uniq\n",
    "    if datatype == 'test':\n",
    "        data_breakfast = []\n",
    "        \n",
    "        segment = []\n",
    "        for content in content_all:\n",
    "            \n",
    "            #file_ptr = open( GT_folder + content, 'r')\n",
    "            #curr_gt = file_ptr.read().split('\\n')[:-1]\n",
    "            \n",
    "            loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n",
    "        \n",
    "            curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n",
    "            feature_name = features_folder + os.path.splitext(content)[0] + '.npy'\n",
    "            np.save(feature_name, curr_data)\n",
    "                                 \n",
    "            data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n",
    "            \n",
    "        \n",
    "        return data_breakfast\n",
    "\n",
    "\n",
    "def get_label_bounds( data_labels):\n",
    "    labels_uniq = []\n",
    "    labels_uniq_loc = []\n",
    "    for kki in range(0, len(data_labels) ):\n",
    "        uniq_group, indc_group = get_label_length_seq(data_labels[kki])\n",
    "        labels_uniq.append(uniq_group[1:-1])\n",
    "        labels_uniq_loc.append(indc_group[1:-1])\n",
    "    return labels_uniq, labels_uniq_loc\n",
    "\n",
    "def get_label_length_seq(content):\n",
    "    label_seq = []\n",
    "    length_seq = []\n",
    "    start = 0\n",
    "    length_seq.append(0)\n",
    "    for i in range(len(content)):\n",
    "        if content[i] != content[start]:\n",
    "            #print(content[i])\n",
    "            label_seq.append(content[start])\n",
    "            length_seq.append(i)\n",
    "            start = i\n",
    "    label_seq.append(content[start])\n",
    "    length_seq.append(len(content))\n",
    "    \n",
    "    if content[-1] != 0:\n",
    "        label_seq.append(content[-1])\n",
    "    \n",
    "    return label_seq, length_seq\n",
    "\n",
    "\n",
    "def get_maxpool_lstm_data(cData, indices):\n",
    "    list_data = []\n",
    "    for kkl in range(len(indices)-1):\n",
    "        cur_start = indices[kkl]\n",
    "        cur_end = indices[kkl+1]\n",
    "        if cur_end > cur_start:\n",
    "            list_data.append(torch.max(cData[cur_start:cur_end,:],\n",
    "                                       0)[0].squeeze(0))\n",
    "        else:\n",
    "            list_data.append(torch.max(cData[cur_start:cur_end+1,:],\n",
    "                                       0)[0].squeeze(0))\n",
    "    list_data  =  torch.stack(list_data)\n",
    "    return list_data\n",
    "\n",
    "def read_mapping_dict(mapping_file):\n",
    "    file_ptr = open(mapping_file, 'r')\n",
    "    actions = file_ptr.read().split('\\n')[:-1]\n",
    "\n",
    "    actions_dict=dict()\n",
    "    for a in actions:\n",
    "        actions_dict[a.split()[1]] = int(a.split()[0])\n",
    "\n",
    "    return actions_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    COMP_PATH = ''\n",
    "    split = 'training'\n",
    "#     split = 'test'\n",
    "    train_split =  os.path.join(COMP_PATH, 'splits/train.split1.bundle')\n",
    "    test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle')\n",
    "    GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/')\n",
    "    DATA_folder =  os.path.join(COMP_PATH, 'data/')\n",
    "    mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt')\n",
    "    train_features_folder = os.path.join(COMP_PATH, 'train_features/')\n",
    "    test_features_folder = os.path.join(COMP_PATH, 'test_features/')\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    actions_dict = read_mapping_dict(mapping_loc)\n",
    "    if  split == 'training':\n",
    "        data_feat, data_labels = load_data(train_split, actions_dict, GT_folder, DATA_folder, train_features_folder, datatype = split)\n",
    "        \n",
    "    if  split == 'test':\n",
    "        data_feat = load_data(test_split, actions_dict, GT_folder, DATA_folder, test_features_folder, datatype = split)\n",
    "    \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = np.random.randint(48, size=(252, 8500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 8500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 15, 26, ...,  0,  2, 27],\n",
       "       [12,  2, 11, ..., 25, 25, 29],\n",
       "       [32, 10, 25, ..., 39, 33, 11],\n",
       "       ...,\n",
       "       [46, 32, 38, ..., 30, 39, 35],\n",
       "       [47, 47, 23, ..., 44, 23, 12],\n",
       "       [ 0, 35, 25, ..., 27, 24, 44]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mapping_dict(mapping_file):\n",
    "    file_ptr = open(mapping_file, 'r')\n",
    "    actions = file_ptr.read().split('\\n')[:-1]\n",
    "\n",
    "    actions_dict=dict()\n",
    "    for a in actions:\n",
    "        actions_dict[a.split()[1]] = int(a.split()[0])\n",
    "\n",
    "    return actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COMP_PATH = ''\n",
    "mapping_file = os.path.join(COMP_PATH, \"data/splits/mapping_bf.txt\")\n",
    "file_ptr = open(mapping_file, 'r')\n",
    "actions = file_ptr.read().split('\\n')[:-1]\n",
    "file_ptr.close()\n",
    "actions_dict = dict()\n",
    "for a in actions:\n",
    "    actions_dict[a.split()[1]] = int(a.split()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SIL': 0,\n",
       " 'add_saltnpepper': 12,\n",
       " 'add_teabag': 45,\n",
       " 'butter_pan': 17,\n",
       " 'crack_egg': 11,\n",
       " 'cut_bun': 36,\n",
       " 'cut_fruit': 32,\n",
       " 'cut_orange': 19,\n",
       " 'fry_egg': 13,\n",
       " 'fry_pancake': 29,\n",
       " 'peel_fruit': 34,\n",
       " 'pour_cereals': 1,\n",
       " 'pour_coffee': 5,\n",
       " 'pour_dough2pan': 28,\n",
       " 'pour_egg2pan': 43,\n",
       " 'pour_flour': 31,\n",
       " 'pour_juice': 21,\n",
       " 'pour_milk': 2,\n",
       " 'pour_oil': 10,\n",
       " 'pour_sugar': 9,\n",
       " 'pour_water': 46,\n",
       " 'put_bunTogether': 40,\n",
       " 'put_egg2plate': 15,\n",
       " 'put_fruit2bowl': 33,\n",
       " 'put_pancake2plate': 30,\n",
       " 'put_toppingOnTop': 39,\n",
       " 'smear_butter': 37,\n",
       " 'spoon_flour': 26,\n",
       " 'spoon_powder': 24,\n",
       " 'spoon_sugar': 7,\n",
       " 'squeeze_orange': 20,\n",
       " 'stir_cereals': 3,\n",
       " 'stir_coffee': 8,\n",
       " 'stir_dough': 27,\n",
       " 'stir_egg': 42,\n",
       " 'stir_fruit': 35,\n",
       " 'stir_milk': 25,\n",
       " 'stir_tea': 47,\n",
       " 'stirfry_egg': 44,\n",
       " 'take_bowl': 4,\n",
       " 'take_butter': 41,\n",
       " 'take_cup': 6,\n",
       " 'take_eggs': 16,\n",
       " 'take_glass': 22,\n",
       " 'take_knife': 18,\n",
       " 'take_plate': 14,\n",
       " 'take_squeezer': 23,\n",
       " 'take_topping': 38}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict_values' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a673cd22cc0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecognition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrecognition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecognition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mactions_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactions_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict_values' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "\n",
    "test_segment =  os.path.join(COMP_PATH, 'test_segment.txt')\n",
    "recognition = []\n",
    "for i in range(len(predicted)):\n",
    "    recognition = np.concatenate((recognition, [actions_dict.keys()[actions_dict.values().index(predicted[i].item())]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_name = vid.split('/')[-1].split('.')[0]\n",
    "f_ptr = open(results_dir + \"/\" + f_name, \"w\")\n",
    "f_ptr.write(\"### Frame level recognition: ###\\n\")\n",
    "f_ptr.write(' '.join(recognition))\n",
    "f_ptr.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
